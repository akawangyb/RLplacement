# 马尔科夫性
马尔可夫性（Markov Property）来自于马尔可夫过程（Markov Process），其基本思想是“无记忆性”。在马尔可夫过程中，未来的状态只取决于当前的状态，而与过去的状态或者过去的行为无关。这被形式地称为“马尔科夫性质”。
对于强化学习，马尔可夫性清晰地反映在马尔可夫决策过程（Markov Decision Process，简称MDP）中。在MDP中，下一个状态 s' 和下一个奖励 r 只依赖于当前的状态 s 和当前的行动 a。
简便地说，它假定环境的下一个状态 s' 只取决于当前的状态 s 和行为 a，而不考虑更早之前的状态和行为。这样的假设使得问题简化，因为对于每个时间步，我们只需要考虑当前的状态和要采取的行动，而不需要考虑任何过去的历史。

# 优先记忆回放
将样本权重加入损失函数是一种常见的做法，目的是为了调整每个样本在模型训练中的影响。
这在权重不均等或数据中存在重要样本的情况下尤其有用。
在你提到的公式loss = torch.mean((Q - Q_target)**2 * weights)里，
(Q - Q_target)**2 计算了预测值 Q 和目标值 Q_target 之间的平方差，
将这个差的平方乘以对应的 weights，就能调整每个样本（或每个预测）在计算总损失时的影响。
具有更大权重的样本会导致损失更大，这反过来会引导模型更加关注这些样本。
换句话说，模型会试图尽可能地减小对这些重要样本预测的误差。

特别是在强化学习中，当使用基于优先级的经验回放（Prioritized Experience Replay）时，
我们希望模型更多地学习那些带有大量信息、或者是错误预测较大的样本。
这样的样本通常比其他样本更具价值，因为它们能在更大程度上提高模型的性能。
在这种情况下，我们会根据样本的优先级（可以通过多种方式来衡量，例如TD错误）来调整样本的权重，
这样模型就会更加注意这些样本。