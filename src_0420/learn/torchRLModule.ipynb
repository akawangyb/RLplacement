{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "强化学习设计用于创建能够有效解决特定任务的策略(policy)。\n",
    "策略可以采取各种形式，从可微的映射(differentiable map)从观察空间到行动空间，到更多随机的方法，如对每个可能行动计算的值列表进行argmax操作。\n",
    "策略可以是确定性(deterministic)的或随机的(stochastic)，可能包含复杂的元素，如递归神经网络（RNNs）或变换器(transformer)。\n",
    "\n",
    "适应所有这些场景可能相当复杂。在这个简洁的教程中，我们将深入讨论TorchRL在策略(policy)构建方面的核心功能。\n",
    "我们将主要关注在两个常见场景下的随机(stochastic)和Q值策略：使用多层感知器（MLP）或卷积神经网络（CNN）作为骨干。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a3515bdbae28174"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TensorDictModules 模块\n",
    "与环境如何与TensorDict实例进行交互类似，这个用于表示策略policy和值函数(value function)的模块也做同样的事情。\n",
    "核心思想很简单：将一个标准的模块(Module)（或任何其他函数）封装在一个类中，这个类知道需要读取(read)和传递(pass)给模块的条目(entry)，并记录分配条目的结果。\n",
    "为了说明这一点，我们将使用最简单的策略：从观察空间到行动空间的确定性映射[类似一个Q表]。\n",
    "为了最大的通用性，我们将使用一个LazyLinear模块，以及我们在上一教程中实例化的Pendulum环境。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77df6635068b7096"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-24T02:24:51.337825200Z",
     "start_time": "2024-04-24T02:24:48.031752700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python39\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torchrl.envs import GymEnv\n",
    "\n",
    "env = GymEnv(\"Pendulum-v1\")\n",
    "module = torch.nn.LazyLinear(out_features=env.action_spec.shape[-1])\n",
    "policy = TensorDictModule(\n",
    "    module,\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"action\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "以上就是执行我们的策略所需的全部！\n",
    "使用懒加载模块（lazy module）可以让我们绕过获取观察空间形状的需求，因为模块会自动决定它。\n",
    "这个策略现在已经准备好在环境中运行了："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b63bfd181d024ca"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T02:25:44.586846300Z",
     "start_time": "2024-04-24T02:25:44.451679100Z"
    }
   },
   "id": "8f07172c4d6d6bda"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Specialized wrappers 定制包装类\n",
    "为了简化  Actor， ProbabilisticActor， ActorValueOperator 或者  ActorCriticOperator的整合。\n",
    "Actor为in_keys和out_keys提供默认值，使得与许多常见环境的整合变得简单直接。[也就是说不用显示指出in-keys和out-keys是什么了]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1266a14fa52ed317"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.modules import Actor\n",
    "\n",
    "policy = Actor(module)\n",
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout)\n",
    "print(policy.in_keys,policy.out_keys)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T02:27:03.772026400Z",
     "start_time": "2024-04-24T02:27:03.717894600Z"
    }
   },
   "id": "67b2de4b5814d99d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 网络\n",
    "TorchRL还提供常规模块，可以在无需依赖于tensordict功能的情况下使用\n",
    "。你会遇到的两个最常见的网络是MLP和ConvNet（CNN）模块。我们可以用这些模块中的一个替换我们的策略模块："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fa94bd5d040fc5a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python39\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "from torchrl.modules import MLP\n",
    "\n",
    "module = MLP(\n",
    "    out_features=env.action_spec.shape[-1],\n",
    "    num_cells=[32, 64],\n",
    "    activation_class=torch.nn.Tanh,\n",
    ")\n",
    "policy = Actor(module)\n",
    "rollout = env.rollout(max_steps=10, policy=policy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T02:27:57.490823100Z",
     "start_time": "2024-04-24T02:27:57.438127600Z"
    }
   },
   "id": "af503cf41550c661"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 这里要搞懂PPO （Proximal Policy Optimization）的原理 [论文地址](https://arxiv.org/abs/1707.06347)\n",
    "像PPO这样的策略优化（policy-optimization）最先进实现要求策略policy是随机的 (stochastic)：\n",
    "与上面的例子不同，模块现在编码了从观察空间到可能的动作分布(the possible actions)的参数空间( parameter space)的映射。\n",
    "TorchRL通过将各种操作如从参数构建分布、从该分布采样和检索对数概率等归入一个类，来简化此类模块的设计。\n",
    "这里，我们将利用三个组件创建一个依赖于常规正态分布的actor\n",
    "1. An MLP backbone reading observations of size [3] and outputting a single tensor of size [2];\n",
    "\n",
    "2. A NormalParamExtractor module that will split this output on two chunks, a mean and a standard deviation of size [1];\n",
    "\n",
    "3. A ProbabilisticActor that will read those parameters as in_keys, create a distribution with them and populate our tensordict with samples and log-probabilities."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49e4e48070c6df40"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        loc: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        sample_log_prob: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        scale: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch.distributions import Normal\n",
    "from torchrl.modules import ProbabilisticActor\n",
    "\n",
    "backbone = MLP(in_features=3, out_features=2)\n",
    "extractor = NormalParamExtractor()\n",
    "module = torch.nn.Sequential(backbone, extractor)\n",
    "td_module = TensorDictModule(module, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"])\n",
    "policy = ProbabilisticActor(\n",
    "    td_module,\n",
    "    in_keys=[\"loc\", \"scale\"],\n",
    "    out_keys=[\"action\"],\n",
    "    distribution_class=Normal,\n",
    "    return_log_prob=True,\n",
    ")\n",
    "\n",
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T02:40:58.481650700Z",
     "start_time": "2024-04-24T02:40:58.349390200Z"
    }
   },
   "id": "7216a489c457cdc9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9beb5b4ac2416b44"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
